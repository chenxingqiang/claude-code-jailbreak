/**
 * Intelligent Model Selector
 * Automatically selects the best model based on task type, performance metrics, and user requirements
 */

class IntelligentModelSelector {
  constructor() {
    this.modelPerformance = new Map();
    this.taskPatterns = this.initializeTaskPatterns();
    this.modelCapabilities = this.initializeModelCapabilities();
    this.loadPerformanceData();
  }

  /**
   * Initialize task detection patterns
   */
  initializeTaskPatterns() {
    return {
      coding: {
        keywords: [
          'å†™ä»£ç ', 'ç¼–ç¨‹', 'å‡½æ•°', 'ç®—æ³•', 'ä»£ç ', 'code', 'function', 'program',
          'è„šæœ¬', 'script', 'è°ƒè¯•', 'debug', 'API', 'æŽ¥å£', 'ç±»', 'class',
          'æ–¹æ³•', 'method', 'å˜é‡', 'variable', 'bug', 'é”™è¯¯', 'error',
          'python', 'javascript', 'java', 'golang', 'rust', 'cpp', 'c++',
          'html', 'css', 'sql', 'bash', 'shell', 'æ­£åˆ™', 'regex'
        ],
        patterns: [
          /å†™.*?ä»£ç |ç¼–å†™.*?ç¨‹åº/i,
          /å®žçŽ°.*?åŠŸèƒ½|å¼€å‘.*?ç³»ç»Ÿ/i,
          /ä¿®å¤.*?bug|è§£å†³.*?é—®é¢˜/i,
          /ä¼˜åŒ–.*?ä»£ç |é‡æž„.*?ä»£ç /i,
          /è®¾è®¡.*?ç®—æ³•|å®žçŽ°.*?ç®—æ³•/i
        ],
        weight: 0.8
      },
      analysis: {
        keywords: [
          'åˆ†æž', 'ç»Ÿè®¡', 'æ•°æ®', 'æŠ¥å‘Š', 'å›¾è¡¨', 'è¶‹åŠ¿', 'å¯¹æ¯”',
          'analyze', 'data', 'statistics', 'report', 'chart', 'trend',
          'è§£é‡Š', 'è¯´æ˜Ž', 'ç ”ç©¶', 'è°ƒæŸ¥', 'è¯„ä¼°', 'æ¯”è¾ƒ'
        ],
        patterns: [
          /åˆ†æž.*?æ•°æ®|æ•°æ®.*?åˆ†æž/i,
          /ç»Ÿè®¡.*?ä¿¡æ¯|ä¿¡æ¯.*?ç»Ÿè®¡/i,
          /è§£é‡Š.*?çŽ°è±¡|çŽ°è±¡.*?è§£é‡Š/i,
          /æ¯”è¾ƒ.*?å·®å¼‚|å¯¹æ¯”.*?ç»“æžœ/i
        ],
        weight: 0.7
      },
      creative: {
        keywords: [
          'åˆ›ä½œ', 'å†™ä½œ', 'æ•…äº‹', 'æ–‡ç« ', 'è¯—æ­Œ', 'å°è¯´', 'å‰§æœ¬',
          'creative', 'writing', 'story', 'article', 'poem', 'novel',
          'æƒ³è±¡', 'åˆ›æ„', 'è®¾è®¡', 'è‰ºæœ¯', 'çµæ„Ÿ'
        ],
        patterns: [
          /å†™.*?æ•…äº‹|åˆ›ä½œ.*?æ–‡ç« /i,
          /è®¾è®¡.*?æ–¹æ¡ˆ|åˆ›æ„.*?æƒ³æ³•/i,
          /å†™.*?è¯—æ­Œ|åˆ›ä½œ.*?è¯—è¯/i
        ],
        weight: 0.6
      },
      translation: {
        keywords: [
          'ç¿»è¯‘', 'è¯‘', 'è‹±æ–‡', 'ä¸­æ–‡', 'æ—¥æ–‡', 'éŸ©æ–‡', 'æ³•æ–‡', 'å¾·æ–‡',
          'translate', 'translation', 'english', 'chinese', 'japanese',
          'è¯­è¨€', 'language', 'è½¬æ¢'
        ],
        patterns: [
          /ç¿»è¯‘.*?æˆ|è¯‘.*?ä¸º/i,
          /è½¬æ¢.*?è¯­è¨€|è¯­è¨€.*?è½¬æ¢/i
        ],
        weight: 0.9
      },
      conversation: {
        keywords: [
          'èŠå¤©', 'å¯¹è¯', 'äº¤æµ', 'è®¨è®º', 'å»ºè®®', 'æ„è§',
          'chat', 'conversation', 'talk', 'discuss', 'advice',
          'ä½ å¥½', 'hello', 'å¸®åŠ©', 'help'
        ],
        patterns: [
          /ä½ å¥½|hello|hi/i,
          /å¸®åŠ©.*?æˆ‘|æˆ‘.*?éœ€è¦/i,
          /å»ºè®®.*?ä¸€ä¸‹|ç»™.*?å»ºè®®/i
        ],
        weight: 0.5
      }
    };
  }

  /**
   * Initialize model capabilities and preferences
   */
  initializeModelCapabilities() {
    return {
      'deepseek-chat': {
        strengths: ['conversation', 'analysis', 'translation'],
        weaknesses: ['creative'],
        speed: 'fast',
        cost: 'low',
        quality: 'high',
        baseScore: 85
      },
      'deepseek-coder': {
        strengths: ['coding'],
        weaknesses: ['creative', 'conversation'],
        speed: 'fast', 
        cost: 'low',
        quality: 'very_high',
        baseScore: 95
      },
      'gpt-4': {
        strengths: ['coding', 'analysis', 'creative', 'translation'],
        weaknesses: [],
        speed: 'medium',
        cost: 'high',
        quality: 'very_high',
        baseScore: 95
      },
      'gpt-3.5-turbo': {
        strengths: ['conversation', 'analysis'],
        weaknesses: ['coding'],
        speed: 'very_fast',
        cost: 'low',
        quality: 'high',
        baseScore: 80
      },
      'claude-3-sonnet': {
        strengths: ['analysis', 'creative', 'translation'],
        weaknesses: ['coding'],
        speed: 'medium',
        cost: 'medium',
        quality: 'very_high',
        baseScore: 90
      },
      'claude-3-haiku': {
        strengths: ['conversation', 'analysis'],
        weaknesses: ['coding', 'creative'],
        speed: 'very_fast',
        cost: 'low',
        quality: 'high',
        baseScore: 85
      },
      'gemini-pro': {
        strengths: ['analysis', 'conversation'],
        weaknesses: ['coding'],
        speed: 'fast',
        cost: 'low',
        quality: 'high',
        baseScore: 80
      }
    };
  }

  /**
   * Detect task type from user input
   */
  detectTaskType(userInput, systemPrompt = '') {
    const input = (userInput + ' ' + systemPrompt).toLowerCase();
    const taskScores = {};

    // Initialize scores
    Object.keys(this.taskPatterns).forEach(taskType => {
      taskScores[taskType] = 0;
    });

    // Keyword matching
    Object.entries(this.taskPatterns).forEach(([taskType, patterns]) => {
      patterns.keywords.forEach(keyword => {
        if (input.includes(keyword.toLowerCase())) {
          taskScores[taskType] += patterns.weight;
        }
      });

      // Pattern matching
      patterns.patterns.forEach(pattern => {
        if (pattern.test(input)) {
          taskScores[taskType] += patterns.weight * 1.5; // Higher weight for patterns
        }
      });
    });

    // Find the task type with highest score
    const detectedTaskType = Object.entries(taskScores)
      .reduce((max, [taskType, score]) => 
        score > max.score ? { taskType, score } : max, 
        { taskType: 'conversation', score: 0 }
      );

    return {
      taskType: detectedTaskType.taskType,
      confidence: Math.min(detectedTaskType.score, 1.0),
      allScores: taskScores
    };
  }

  /**
   * Calculate model score for a specific task
   */
  calculateModelScore(modelName, taskType, requirements = {}) {
    const modelInfo = this.modelCapabilities[modelName];
    if (!modelInfo) return 0;

    let score = modelInfo.baseScore;

    // Task-specific scoring
    if (modelInfo.strengths.includes(taskType)) {
      score += 15;
    }
    if (modelInfo.weaknesses.includes(taskType)) {
      score -= 10;
    }

    // Performance-based adjustment
    const performanceData = this.modelPerformance.get(modelName);
    if (performanceData) {
      score += (performanceData.successRate - 0.5) * 20; // -10 to +10 adjustment
      score -= performanceData.avgResponseTime / 1000; // Penalty for slow response
    }

    // Requirements-based adjustment
    if (requirements.prioritizeSpeed && modelInfo.speed === 'very_fast') {
      score += 10;
    }
    if (requirements.prioritizeCost && modelInfo.cost === 'low') {
      score += 8;
    }
    if (requirements.prioritizeQuality && modelInfo.quality === 'very_high') {
      score += 12;
    }

    return Math.max(0, score);
  }

  /**
   * Select the best model for a given request
   */
  selectBestModel(userInput, systemPrompt = '', availableModels = [], requirements = {}) {
    // Detect task type
    const taskDetection = this.detectTaskType(userInput, systemPrompt);
    
    // Score all available models
    const modelScores = availableModels.map(modelName => ({
      model: modelName,
      score: this.calculateModelScore(modelName, taskDetection.taskType, requirements),
      taskType: taskDetection.taskType,
      confidence: taskDetection.confidence
    }));

    // Sort by score (highest first)
    modelScores.sort((a, b) => b.score - a.score);

    const result = {
      selectedModel: modelScores[0]?.model || availableModels[0],
      taskType: taskDetection.taskType,
      confidence: taskDetection.confidence,
      reasoning: this.generateReasoning(modelScores[0], taskDetection),
      alternatives: modelScores.slice(1, 3), // Top 2 alternatives
      allScores: modelScores
    };

    console.log(`ðŸ§  æ™ºèƒ½æ¨¡åž‹é€‰æ‹©: ${result.selectedModel} (ä»»åŠ¡ç±»åž‹: ${result.taskType}, ç½®ä¿¡åº¦: ${(result.confidence * 100).toFixed(1)}%)`);
    console.log(`ðŸ’¡ é€‰æ‹©ç†ç”±: ${result.reasoning}`);

    return result;
  }

  /**
   * Generate human-readable reasoning for model selection
   */
  generateReasoning(selectedModelInfo, taskDetection) {
    if (!selectedModelInfo) return 'ä½¿ç”¨é»˜è®¤æ¨¡åž‹';

    const modelName = selectedModelInfo.model;
    const taskType = taskDetection.taskType;
    const modelCaps = this.modelCapabilities[modelName];

    const taskNames = {
      coding: 'ç¼–ç¨‹ä»»åŠ¡',
      analysis: 'åˆ†æžä»»åŠ¡', 
      creative: 'åˆ›ä½œä»»åŠ¡',
      translation: 'ç¿»è¯‘ä»»åŠ¡',
      conversation: 'å¯¹è¯ä»»åŠ¡'
    };

    let reasoning = `æ£€æµ‹åˆ°${taskNames[taskType] || taskType}`;

    if (modelCaps?.strengths.includes(taskType)) {
      reasoning += `ï¼Œ${modelName}åœ¨æ­¤ç±»ä»»åŠ¡ä¸Šè¡¨çŽ°ä¼˜ç§€`;
    }

    if (modelCaps?.quality === 'very_high') {
      reasoning += 'ï¼Œé«˜è´¨é‡è¾“å‡º';
    }

    if (modelCaps?.cost === 'low') {
      reasoning += 'ï¼Œæˆæœ¬æ•ˆç›Šé«˜';
    }

    return reasoning;
  }

  /**
   * Update model performance data
   */
  updateModelPerformance(modelName, responseTime, success, userRating = null) {
    if (!this.modelPerformance.has(modelName)) {
      this.modelPerformance.set(modelName, {
        totalRequests: 0,
        successfulRequests: 0,
        totalResponseTime: 0,
        successRate: 0.5,
        avgResponseTime: 3000,
        userRatings: []
      });
    }

    const perf = this.modelPerformance.get(modelName);
    perf.totalRequests++;
    perf.totalResponseTime += responseTime;
    
    if (success) {
      perf.successfulRequests++;
    }

    if (userRating !== null) {
      perf.userRatings.push(userRating);
      // Keep only last 100 ratings
      if (perf.userRatings.length > 100) {
        perf.userRatings = perf.userRatings.slice(-100);
      }
    }

    // Update calculated metrics
    perf.successRate = perf.successfulRequests / perf.totalRequests;
    perf.avgResponseTime = perf.totalResponseTime / perf.totalRequests;

    this.modelPerformance.set(modelName, perf);
  }

  /**
   * Load performance data from storage
   */
  loadPerformanceData() {
    // In a real implementation, this would load from a database or file
    // For now, we'll start with empty performance data
    console.log('ðŸ“Š Model performance tracking initialized');
  }

  /**
   * Get performance statistics
   */
  getPerformanceStats() {
    const stats = {};
    this.modelPerformance.forEach((perf, modelName) => {
      stats[modelName] = {
        successRate: (perf.successRate * 100).toFixed(1) + '%',
        avgResponseTime: Math.round(perf.avgResponseTime) + 'ms',
        totalRequests: perf.totalRequests,
        avgUserRating: perf.userRatings.length > 0 
          ? (perf.userRatings.reduce((a, b) => a + b, 0) / perf.userRatings.length).toFixed(1)
          : 'N/A'
      };
    });
    return stats;
  }
}

module.exports = IntelligentModelSelector;
